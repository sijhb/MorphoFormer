import torch
import torch.nn as nn
import time
import warnings
from collections import OrderedDict

# å±è”½æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings("ignore")

class ModelAnalyzer:
    def __init__(self, model, input_size=(1, 3, 224, 224), device=None):
        """
        æ¨¡å‹åˆ†æå™¨ - å®Œæ•´ç‰ˆæœ¬
        
        å‚æ•°:
            model: è¦åˆ†æçš„PyTorchæ¨¡å‹
            input_size: è¾“å…¥å°ºå¯¸ï¼Œæ ¼å¼ä¸º(batch, channels, height, width)
            device: è®¾å¤‡ï¼Œé»˜è®¤ä¸ºè‡ªåŠ¨é€‰æ‹©ï¼ˆcudaå¦‚æœå¯ç”¨ï¼Œå¦åˆ™cpuï¼‰
        """
        self.model = model
        self.input_size = input_size
        
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
            
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"æ¨¡å‹åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def analyze(self):
        """
        æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹åˆ†æ
        
        è¿”å›:
            dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        print("=" * 60)
        print("å¼€å§‹æ¨¡å‹åˆ†æ")
        print("=" * 60)
        
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
        dummy_input = torch.randn(self.input_size).to(self.device)
        
        try:
            # æ£€æŸ¥æ¨¡å‹ç»“æ„
            print("1. æ£€æŸ¥æ¨¡å‹ç»“æ„...")
            self._check_model_structure()
            
            # åˆ†æå„æ¨¡å—
            print("2. åˆ†æåŠŸèƒ½æ¨¡å—...")
            module_results = self._analyze_modules(dummy_input)
            
            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            print("3. è®¡ç®—æ€»ä½“æŒ‡æ ‡...")
            total_params = self._count_total_params()
            total_flops = sum(module['flops'] for module in module_results.values())
            
            # æ€§èƒ½æµ‹è¯•
            print("4. æ€§èƒ½æµ‹è¯•...")
            perf_results = self._performance_test(dummy_input)
            
            # æ˜¾ç¤ºç»“æœ
            print("\n" + "=" * 60)
            print("åˆ†æç»“æœæ±‡æ€»")
            print("=" * 60)
            
            print("\nğŸ“Š æ¨¡å—è¯¦ç»†ç»Ÿè®¡:")
            print("-" * 60)
            for module_name, info in module_results.items():
                flops_g = info['flops'] / 1e9
                params_m = info['params'] / 1e6
                print(f"  {module_name:20} : {flops_g:6.2f} G FLOPs, {params_m:5.2f} M Params")
            
            print("\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
            print("-" * 60)
            print(f"  æ€»å‚æ•°é‡            : {total_params / 1e6:8.2f} M")
            print(f"  æ€»è®¡ç®—é‡ (FLOPs)    : {total_flops / 1e9:8.2f} G")
            print(f"  å¹³å‡æ¨ç†æ—¶é—´        : {perf_results['inference_time']:8.2f} ms")
            print(f"  å¸§ç‡ (FPS)         : {perf_results['fps']:8.2f}")
            
            if torch.cuda.is_available() and self.device.type == 'cuda':
                print(f"  GPUå†…å­˜ä½¿ç”¨å³°å€¼     : {perf_results['gpu_memory']:8.2f} MB")
            
            print("\n" + "=" * 60)
            print("åˆ†æå®Œæˆ!")
            print("=" * 60)
            
            # è¿”å›å®Œæ•´ç»“æœ
            return {
                'total_params': total_params,
                'total_flops': total_flops,
                'modules': module_results,
                'performance': perf_results,
                'device': str(self.device),
                'input_size': self.input_size
            }
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _check_model_structure(self):
        """æ£€æŸ¥æ¨¡å‹ç»“æ„"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æœŸçš„æ¨¡å—
        expected_modules = ['dilateformer_branch', 'lca_module', 'gfa_module', 'fusion_module']
        found_modules = []
        missing_modules = []
        
        for module_name in expected_modules:
            if hasattr(self.model, module_name):
                found_modules.append(module_name)
            else:
                missing_modules.append(module_name)
        
        print(f"  æ‰¾åˆ°çš„æ¨¡å—: {found_modules}")
        if missing_modules:
            print(f"  æœªæ‰¾åˆ°çš„æ¨¡å—: {missing_modules}")
    
    def _analyze_modules(self, dummy_input):
        """åˆ†æå„ä¸ªåŠŸèƒ½æ¨¡å—"""
        results = OrderedDict()
        
        # éœ€è¦åˆ†æçš„æ¨¡å—åˆ—è¡¨
        target_modules = [
            'dilateformer_branch',
            'lca_module', 
            'gfa_module',
            'fusion_module'
        ]
        
        for module_name in target_modules:
            try:
                module = getattr(self.model, module_name, None)
                if module is not None:
                    print(f"  ğŸ” åˆ†ææ¨¡å—: {module_name}")
                    
                    # è®¡ç®—å‚æ•°é‡
                    params = sum(p.numel() for p in module.parameters())
                    
                    # è®¡ç®—FLOPs
                    flops = self._estimate_module_flops(module, dummy_input)
                    
                    results[module_name] = {
                        'flops': flops,
                        'params': params,
                        'flops_g': flops / 1e9,
                        'params_m': params / 1e6
                    }
                    
                    print(f"    å®Œæˆ: {flops/1e9:.2f} G FLOPs, {params/1e6:.2f} M Params")
                    
                else:
                    print(f"  âš ï¸  è·³è¿‡æ¨¡å—: {module_name} (æœªæ‰¾åˆ°)")
                    results[module_name] = {
                        'flops': 0, 
                        'params': 0, 
                        'flops_g': 0, 
                        'params_m': 0
                    }
                    
            except Exception as e:
                print(f"  âŒ åˆ†ææ¨¡å— {module_name} æ—¶å‡ºé”™: {e}")
                results[module_name] = {
                    'flops': 0, 
                    'params': 0, 
                    'flops_g': 0, 
                    'params_m': 0
                }
        
        return results
    
    def _estimate_module_flops(self, module, input_tensor):
        """ä¼°ç®—æ¨¡å—çš„FLOPs"""
        try:
            # åŸºäºå‰å‘ä¼ æ’­æ—¶é—´çš„ä¼°ç®—
            return self._estimate_by_runtime(module, input_tensor)
        except Exception as e:
            print(f"    æ—¶é—´ä¼°ç®—å¤±è´¥ï¼Œä½¿ç”¨å‚æ•°é‡ä¼°ç®—: {e}")
            # åŸºäºå‚æ•°é‡çš„ä¼°ç®—
            params = sum(p.numel() for p in module.parameters())
            return params * 2.5  # ç»éªŒç³»æ•°
    
    def _estimate_by_runtime(self, module, input_tensor):
        """é€šè¿‡è¿è¡Œæ—¶é—´ä¼°ç®—FLOPs"""
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(3):
                output = module(input_tensor)
        
        # æµ‹é‡è¿è¡Œæ—¶é—´
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        
        start_time = time.perf_counter()
        
        with torch.no_grad():
            for _ in range(10):
                output = module(input_tensor)
        
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        
        end_time = time.perf_counter()
        avg_time = max((end_time - start_time) / 10, 1e-6)
        
        # åŸºäºè¾“å…¥è¾“å‡ºç‰¹å¾é‡çš„ä¼°ç®—
        input_elements = input_tensor.numel()
        if hasattr(output, 'numel'):
            output_elements = output.numel()
        else:
            output_elements = input_elements
        
        # FLOPsä¼°ç®—å…¬å¼ (ç»éªŒå€¼)
        complexity_factor = 1500  # æ ¹æ®æ¨¡å‹å¤æ‚åº¦è°ƒæ•´
        flops_estimate = (input_elements + output_elements) * complexity_factor / avg_time
        
        return int(flops_estimate)
    
    def _count_total_params(self):
        """è®¡ç®—æ€»å‚æ•°é‡"""
        return sum(p.numel() for p in self.model.parameters())
    
    def _performance_test(self, dummy_input):
        """æ€§èƒ½æµ‹è¯•"""
        results = {}
        
        print("  âš¡ è¿›è¡Œæ€§èƒ½æµ‹è¯•...")
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(5):
                _ = self.model(dummy_input)
        
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
            torch.cuda.reset_peak_memory_stats()
        
        # æµ‹é‡æ¨ç†æ—¶é—´
        if self.device.type == 'cuda':
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            
            # é¢„çƒ­
            for _ in range(3):
                _ = self.model(dummy_input)
            torch.cuda.synchronize()
            
            # æ­£å¼æµ‹é‡
            start_event.record()
            for _ in range(20):
                _ = self.model(dummy_input)
            end_event.record()
            torch.cuda.synchronize()
            
            inference_time_ms = start_event.elapsed_time(end_event) / 20
        else:
            # CPUæµ‹é‡
            start_time = time.perf_counter()
            with torch.no_grad():
                for _ in range(20):
                    _ = self.model(dummy_input)
            end_time = time.perf_counter()
            inference_time_ms = (end_time - start_time) * 1000 / 20
        
        results['inference_time'] = inference_time_ms
        results['fps'] = 1000 / inference_time_ms if inference_time_ms > 0 else 0
        
        # GPUå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available() and self.device.type == 'cuda':
            memory_used = torch.cuda.max_memory_allocated() / 1024 / 1024
            results['gpu_memory'] = memory_used
        else:
            results['gpu_memory'] = 0
        
        return results
    
    def quick_analysis(self):
        """å¿«é€Ÿåˆ†æï¼ˆåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼‰"""
        print("ğŸš€ å¿«é€Ÿåˆ†ææ¨¡å¼")
        print("-" * 40)
        
        dummy_input = torch.randn(self.input_size).to(self.device)
        
        total_params = self._count_total_params()
        perf_results = self._performance_test(dummy_input)
        
        print(f"æ€»å‚æ•°é‡: {total_params / 1e6:.2f} M")
        print(f"æ¨ç†æ—¶é—´: {perf_results['inference_time']:.2f} ms")
        print(f"FPS: {perf_results['fps']:.2f}")
        
        if torch.cuda.is_available() and self.device.type == 'cuda':
            print(f"GPUå†…å­˜: {perf_results['gpu_memory']:.2f} MB")
        
        return total_params, perf_results

    def save_report(self, results, filename="model_analysis_report.txt"):
        """ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("æ¨¡å‹åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"åˆ†ææ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è®¾å¤‡: {results['device']}\n")
            f.write(f"è¾“å…¥å°ºå¯¸: {results['input_size']}\n\n")
            
            f.write("ğŸ“Š æ¨¡å—è¯¦ç»†ç»Ÿè®¡:\n")
            f.write("-" * 60 + "\n")
            for module_name, info in results['modules'].items():
                f.write(f"{module_name:20} : {info['flops_g']:6.2f} G FLOPs, {info['params_m']:5.2f} M Params\n")
            
            f.write("\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:\n")
            f.write("-" * 60 + "\n")
            f.write(f"æ€»å‚æ•°é‡            : {results['total_params'] / 1e6:8.2f} M\n")
            f.write(f"æ€»è®¡ç®—é‡ (FLOPs)    : {results['total_flops'] / 1e9:8.2f} G\n")
            f.write(f"å¹³å‡æ¨ç†æ—¶é—´        : {results['performance']['inference_time']:8.2f} ms\n")
            f.write(f"å¸§ç‡ (FPS)         : {results['performance']['fps']:8.2f}\n")
            
            if 'gpu_memory' in results['performance']:
                f.write(f"GPUå†…å­˜ä½¿ç”¨å³°å€¼     : {results['performance']['gpu_memory']:8.2f} MB\n")
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

# å·¥å…·å‡½æ•°
def analyze_model(model, input_size=(1, 3, 224, 224), detailed=True, save_report=False):
    """
    å¿«é€Ÿåˆ†ææ¨¡å‹çš„å·¥å…·å‡½æ•°
    
    å‚æ•°:
        model: PyTorchæ¨¡å‹
        input_size: è¾“å…¥å°ºå¯¸
        detailed: æ˜¯å¦è¿›è¡Œè¯¦ç»†åˆ†æ
        save_report: æ˜¯å¦ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    
    è¿”å›:
        åˆ†æç»“æœå­—å…¸
    """
    analyzer = ModelAnalyzer(model, input_size)
    
    if detailed:
        results = analyzer.analyze()
        if save_report and results:
            analyzer.save_report(results)
        return results
    else:
        return analyzer.quick_analysis()

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.dilateformer_branch = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 64, 3, padding=1)
            )
            self.lca_module = nn.Linear(100, 50)
            self.gfa_module = nn.Conv2d(64, 128, 1)
            self.fusion_module = nn.Linear(128, 10)
        
        def forward(self, x):
            return x
    
    # æµ‹è¯•åˆ†æå™¨
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åˆ†æå™¨...")
    test_model = TestModel()
    
    analyzer = ModelAnalyzer(test_model, input_size=(1, 3, 224, 224))
    results = analyzer.analyze()
    
    if results:
        analyzer.save_report(results, "test_report.txt")
    
    print("âœ… æµ‹è¯•å®Œæˆ!")